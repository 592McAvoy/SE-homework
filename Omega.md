# omega

### **1. 背景**

Google的第一代/第二代集群（资源）管理系统被称为Borg，Borg设计细节因零零星星出现在各种文章中而知名，但一直未公开（比如发一篇paper）。之后，Google公布了它的下一代集群管理系统Omega）的设计细节。Google经历的三代资源调度器的架构，分别是中央式调度器架构（类似于Hadoop JobTracker，但是支持多种类型作业调度）、双层调度器架构（类似于Apache Mesos)和Hadoop YARN）和共享状态架构（就是Omega）。Omega整体架构与现有的资源管理系统，比如Apache Mesos，非常类似（比如各个slave上会部署一个代理用户接收任务，向master汇报任务状态和资源使用情况等），其主要不同集中在资源管器上。

### 

### **2. 集群管理（或者叫资源管理）系统的设计动机**

集群资源管理系统是对底层硬件的进一步抽象，它屏蔽了硬件的异构性，对上层各种应用提供资源统一管理和调度。从当前公认的云计算划分看，它属于IAAS（Infrastructure-as-a- Service）。这类系统的设计动机，主要有两个，分别是提高系统利用率和服务自动化部署，google在Omega也正是基于此。

这类系统不同于现在的Hadoop，Hadoop运行的任务是快短类型的，可以运行在任何很烂的机器上，一旦任务失败后，可以很快地将之调度运行到另外一个机器上；而类似于Omega或者Mesos的资源管理系统则不同，它不仅要运行这种短类型的任务，更多的是运行一些长类型的服务，比如web service、MySQL Server等，对于这类服务，Omega应尽量将其调度到一个性能稳定可靠的节点上，这通常是通过跟踪每个节点的历史表现情况判断节点的稳定性和可靠性实现的，比如，如果你向通过Omega运行一个大约工作1个月的web service（一个月后可能会弃用），那么，Omega会通过分析历史数据，得到一个月内出现故障的可能性最低的节点，并将该节点的资源分配给该web service，而对于一个MapReduce作业，可将任何节点分配给他，但从资源合理使用上看，应尽可能将一些表现差的节点分配给MapReduce作业或者一些性能好的节点上的琐碎资源分配给它。



### **3.  三类集群管理系统**

Google经历的三代资源管理系统，这三代系统分别如下：

![three-types-of-schedulers](http://dongxicheng.org/wp-content/uploads/2013/04/three-types-of-schedulers.jpg)

##### **（1） 中央式调度器（Monolithic scheduler）**

中央式调度器的特点是，资源的调度和作业的管理功能全部放到一个进程中完成，开源界典型的代表是Hadoop JobTracker的实现。这种设计方式的缺点很明显，扩展性差：首先，集群规模受限，其次，新的调度策略难以融入现有代码中，比如之前仅支持MapReduce作业，现在要支持流式作业，而将流式作业的调度策略嵌入到中央式调度器中是一项很难的工作。

Omega中提到了一种对中央式调度器的优化方案：将每种调度策略放到单独一个路径（模块）中，不同的作业由不同的调度策略进行调度。这种方案在作业量和集群规模比较小时，能大大缩短作业相应时间，但由于所有调度策略仍在一个集中式的组件中，整个系统扩展性没有变得更好。

##### **（2）  双层调度器（Two-level scheduler）**

为了解决中央式调度器的不足，双层调度器是一种很容易想到的解决之道（实际上是分而治之策略或者是策略下放机制）。双层调度器仍保留一个经简化的中央式调度器，但调度策略下放到各个应用程序调度器完成。这种调度器的典型代表是Apache Mesos和Hadoop YARN。Omega论文重点介绍了Mesos，Mesos是twitter开源的资源管理系统：

Mesos资源管理部分由两部分组成：分别是Mesos Master和Mesos Slave，其中，Mesos Slave是每个节点上的代理，负责向Master汇报信息和接收并执行来自Master的命令，而Master则是一个轻量级中央化的资源管理器，负责管理和分配整个集群中的资源。如果一个应用程序想通过Mesos资源管理系统申请和使用资源，需编写两个组件：框架调度器和框架执行器，其中，框架调度器负责从Mesos Master上获取资源、将资源分配给自己内部的各个应用程序，并控制应用程序的执行过程；而框架执行器运行在Mesos Slave中，负责运行该框架中的任务。当前很多框架可以接入Mesos中，包括Hadoop、MPI、Spark等。

![mesos-arch](http://dongxicheng.org/wp-content/uploads/2012/04/mesos-arch.jpg)

双层调度器的特点是，各个框架调度器并不知道整个集群资源使用情况，只是被动的接收资源；Mesos Master仅将可用的资源推送给各个框架，而框架自己选择使用还是拒绝这些资源；一旦框架（比如Hadoop JobTracker）接收到新资源后，再进一步将资源分配给其内部的各个应用程序（各个MapReduce作业），进而实现双层调度。

双层调度器的缺点是：

###### **1）  各个框架无法知道整个集群的实时资源使用情况。**

很多框架不需要知道整个集群的实时资源使用情况就可以运行的很顺畅，但是对于其他一些应用，为之提供实时资源使用情况可以为之提供潜在的优化空间，比如，当集群非常繁忙时，一个服务失败了，是选择换一个节点重新运行它呢，还是继续在这个节点上运行？通常而言，换一个节点可能会更有利，但是，如果此时集群非常繁忙，所有节点只剩下小于5GB的内存，而这个服务需要10GB内存，那么换一个节点可能意味着长时间等待资源释放，而这个等待时间是无法确定的。

###### **2）  采用悲观锁，并发粒度小。**

在数据库领域，悲观锁与乐观锁争论一直不休，悲观锁通常采用锁机制控制并发，这会大大降低性能，而乐观锁则采用多版本并发控制(MVCC ,Multi-Version Concurrency Control)，典型代表是MySQL innoDB，这种机制通过多版本方式控制并发，可大大提升性能。在Mesos中，在任意一个时刻，Mesos资源调度器只会将所有资源推送给任意一个框架，等到该框架返回资源使用情况后，才能够将资源推动给其他框架，因此，Mesos资源调度器中实际上有一个全局锁，这大大限制了系统并发性。

##### **（3） 共享状态调度器（Shared State Scheduler）**

为了克服双层调度器的以上两个缺点，Google开发了下一代资源管理系统Omega，Omega是一种基于共享状态的调度器，该调度器将双层调度器中的集中式资源调度模块简化成了一些持久化的共享数据（状态）和针对这些数据的验证代码，而这里的“共享数据”实际上就是整个集群的实时资源使用信息。一旦引入共享数据后，共享数据的并发访问方式就成为该系统设计的核心，而Omega则采用了传统数据库中基于多版本的并发访问控制方式（也称为“乐观锁”, MVCC, Multi-Version Concurrency Control），这大大提升了Omega的并发性。

由于Omega不再有集中式的调度模块，因此，不能像Mesos或者YARN那样，在一个统一模块中完成以下功能：对整个集群中的所有资源分组，限制每类应用程序的资源使用量，限制每个用户的资源使用量等，这些全部由各个应用程序调度器自我管理和控制，根据论文所述，Omega只是将优先级这一限制放到了共享数据的验证代码中，即当同时由多个应用程序申请同一份资源时，优先级最高的那个应用程序将获得该资源，其他资源限制全部下放到各个子调度器。

引入多版本并发控制后，限制该机制性能的一个因素是资源访问冲突的次数，冲突次数越多，系统性能下降的越快，而google通过实际负载测试证明，这种方式的冲突次数是完全可以接受的。

###### 1）乐观与冲突

Omega让资源邀约更进一步。在Mesos中，资源邀约是悲观的或独占的。如果资源已经提供给一个应用程序，同样的资源将不能提供给另一个应用程序，直到邀约超时。在Omega中，资源邀约是乐观的。每个应用程序可以请求群集上的所有可用资源，冲突在提交时解决。Omega的资源管理器基本上只是一个记录每个节点状态的关系数据库，使用不同类型的乐观并发控制解决冲突。这样的好处是大大增加了调度器的性能（完全并行）和更好的利用率。

所有这一切的缺点是，应用程序是在一个绝对自由的环境中，他们可以以最快的速度吞噬他们想要的资源，甚至抢占其他用户的资源。对谷歌来说这没问题，因为他们使用基于优先级的系统，并且可以向他们的内部用户咆哮。他们的应用大致只分为两种优先级：高优先级的服务性作业（如HBase、web服务器、长住服务等）和低优先级的批处理作业（MapReduce和类似技术）。应用程序可以抢占低优先级的作业，并且在协作执行限制的范围＃内授信，以提交作业、计算资源分配等。我认为雅虎没法冲着用户大叫（当然是不可扩展的），但这在谷歌某种方式上是可行的。

乐观的分配方案如何与冲突一起工作的，这永远是个问题。有一些高级的注意事项：

- 服务性作业都较大，对（跨机架的）容错有更严格的配置需求。
- 由于在分配完全群集状态上的开销，Omega大概可以将调度器扩展到十倍，但是无法达到百倍。
- 秒级的调度时间是典型的。他们还比较了十秒级和百秒级的调度，这是两级调度的好处真正发挥作用的地方。无法确定这样的场景有多普遍，也许是由服务性作业来决定？
- 典型的集群利用率约为60％。
- 在OCC实践中，冲突非常罕见。在调度器崩溃之前，他们能够将正常批处理作业上升6倍。
- 增量调度是非常重要的。组调度明显更昂贵，因为要增加冲突处理的实现。显然，大多数的应用程序可以做好增量，通过实现部分资源分配进而达到他们所需的全额资源。
- 即使执行复杂的调度器（每作业十余秒的费），Omega仍然可以在合理的等待时间内调度一个混合的作业。
- 用一个新的MapReduce调度器进行实验，从经验上说，在Omega中会非常容易。

###### 2）开放式问题

- 在某些时候，因为高冲突率和重试导致的重复工作，乐观并发控制会崩溃。在实践中他们好像没有碰到这种问题，但我不知道是否有奇形怪状的任务致使出现最坏的场景。这是受服务和批处理作业联合影响的吗？在实践中他们做过什么调优吗？
- 是否缺乏真正可以接受的全局策略？如公平性、抢占等。
- 不同类型的作业调度的时间是多少？有人已经写出非常复杂的调度器吗？



### **4. 资源分配**

当前集群管理系统有两种资源分配方式，分别是：“all-or-nothing”和“incremental placement”，在此举例说明：一个任务需要2GB内存，而一个节点剩余1GB，若将这1GB内存分配给该任务，则需等待将节点释放另外1GB内存才可运行该任务，这种方式称为“incremental placement”，Hadoop YARN采用了这种增量资源分配的方式，而如果只为该任务选择剩余节点超过2GB内存的节点，其他不考虑，则称为“all-or-nothing”，Mesos和Omega均采用了这种方式。两种方式各有优缺点，“all-or-nothing”可能会造成作业饿死（大资源需求的任务永远得到不需要的资源），而“incremental placement”会造成资源长时间闲置，同时可也能导致作业饿死，比如一个服务需要10GB内存，当前一个节点上剩余8GB，调度器将这些资源分配给它并等待其他任务释放2GB，然而，由于其他任务运行时间非常长，可能短时间内不会释放，这样，该服务将长时间得不到运行。



### 5、调度

##### （1）基于位置的调度

超级计算机将存储和计算分离，并使用近似于全等分带宽的网络连接起来，其运行速度接近内存速度（GB/秒）。这意味着我们的任务可以放置在集群上的任何地方，而不必考虑具体位置，因为所有计算节点都可以同样快速地访问数据。一些超优化的应用优化了网络拓扑，但这毕竟是非常罕见的。数据中心调度系统会关心具体位置，实际上，这是GFS和MapReduce共同设计的结果。回眸2000年初，那时的网络带宽远比磁盘带宽昂贵。所以，为了经济上极大的节约，调度算法会将计算任务保持在与数据相同的节点上。这是调度主要的约束;在我们可以把任务放到任何地方之前，需要将其放在三个数据副本之一的节点上。

##### （2）队列管理与调度

在超级计算机上运行应用程序时，你需要指定想要多少个节点、要提交作业的队列，以及作业将运行多长时间。队列存储可以请求多少资源、作业可以运行多久等不同的限制。同时，队列也有优先级或基于系统的预留来确定排序。由于作业的持续时间都是已知的，这是一个非常简单的打包到盒子的问题。如果队列很长（通常就是这样），并有可以很好融合的小作业回填大作业（也是典型）的剩余空间，则可以实现非常高的资源利用率。我很喜欢将这一描述用以时间为X轴，以资源使用为Y轴的可视化2D图呈现出来。

如前所述，数据中心的调度是一个比较普遍的问题。资源请求的“形状”可以是相当多样，并有更多的维度。作业也没有设定持续时间，所以很难预先计划队列。因此，我们有更复杂的调度算法，从而，调度器的性能变得非常重要。

利用率作为一般的规则将变得糟糕（除非你是谷歌，以及更多的后来者），但高性能计算之上的应用有一个优势是可以使用MapReduce和类似的技术逐渐代替组调度。高性能计算，会等到请求所需的N个节点都可用后，同时运行所有任务。MapReduce可以在多次波动中运行它的任务，这意味着它仍然可以有效地利用剩余的资源。单一的MR作业也可以基于集群的需求起伏，避免了抢占或资源预留，并且还有助于实现多用户之间的公平性。



### 6、优缺点

##### （1）优点

共享资源状态，支持更大的集群和更高的并发。

##### （2）缺点

只有论文，无具体实现，在小集群下，没有优势。



### 7、结语与展望

Google十多年搭建容器管理系统的积累了很多经验教。而Google开发者把很多已有的经验融入进了Omega，谷歌最近的这个容器管理系统。它的目标是基于容器的能力来提供编程生产力方面的极大收获，简化人工和自动化系统管理。

通过对Google Omega论文的研读，我想知道，是否谷歌的假设作业更具备普遍性。谷歌能很好地使用优先级分类、资源预留，以及抢占，但是用户几乎全部使用公平共享调度器。雅虎使用容量调度器。 Twitter使用的公平调度器。没有听说过的关于优先级+资源预留调度器的任何需求或使用。最后，很少有用户会运行在谷歌预想的大共享集群上。用户在使用千级的节点，但是这被分为百级的pod节点上。为单独的用户或者应用程序独立部署集群也是常见的。集群在硬件方面通常还是同构的。我认为这将开始改变，并且会很快。

最后我希望大型集群管理系统能够尽快普及化，而不是停留在理论层次，或者不友好的商业模式。