# xPU：百花齐放的AI芯片

### 概述：

现在这年代，技术日新月异，物联网、人工智能、深度学习等概念遍地开花，各类芯片名词GPU, TPU, NPU，DPU层出不穷。为了让市场和观众能记住自家的产品，各家在芯片命名方面都下了点功夫，既要独特，又要和公司产品契合，还要朗朗上口，也要容易让人记住。比较有意思的是，很多家都采用了“xPU”的命名方式。下面我将简单的介绍一下不同“xPU"之间区别与所擅长的领域。



### 四大金花：

#### 经典大脑：CPU

CPU( Central Processing Unit, 中央处理器)就是机器的“大脑”，也是布局谋略、发号施令、控制行动的“总司令官”。

CPU的结构主要包括运算器（ALU, Arithmetic and Logic Unit）、控制单元（CU, Control Unit）、寄存器（Register）、高速缓存器（Cache）和它们之间通讯的数据、控制及状态的总线。

简单来说就是：计算单元、控制单元和存储单元，架构如下图所示：

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/e664ed39638648469655f0fefe19f731.png)

图：CPU微架构示意图

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/693a2124753a4754b9a0bb44d3089c1f.png)

图：CPU微架构示意图

从字面上我们也很好理解，**计算单元**主要执行算术运算、移位等操作以及地址运算和转换；**存储单元**主要用于保存运算中产生的数据以及指令等；**控制单元**则对指令译码，并且发出为完成每条指令所要执行的各个操作的控制信号。

所以一条指令在CPU中执行的过程是这样的：读取到指令后，通过指令总线送到控制器（黄色区域）中进行译码，并发出相应的操作控制信号；然后运算器（绿色区域）按照操作指令对数据进行计算，并通过数据总线将得到的数据存入数据缓存器（大块橙色区域）。过程如下图所示：

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/636a564e61c140bbbf2826cc9cbca527.jpeg)

图：CPU执行指令图

CPU遵循的是冯诺依曼架构，其核心就是：存储程序，顺序执行。因为CPU的架构中需要大量的空间去放置**存储单元（橙色部分）**和**控制单元（黄色部分）**，相比之下**计算单元（绿色部分）**只占据了很小的一部分，所以它在大规模并行计算能力上极受限制，而更擅长于逻辑控制。

因为遵循冯诺依曼架构（存储程序，顺序执行），CPU就像是个一板一眼的管家，人们吩咐的事情它总是一步一步来做。但是随着人们对更大规模与更快处理速度的需求的增加，这位管家渐渐变得有些力不从心。



#### 图像能手：GPU

基于CPU效率不高的缺点，大家就想，能不能把多个处理器放在同一块芯片上，让它们一起来做事，这样效率不就提高了吗？这样，GPU便由此诞生了，GPU的优势就是并行计算。

并行计算(Parallel Computing)是指同时使用多种计算资源解决计算问题的过程，是提高计算机系统计算速度和处理能力的一种有效手段。它的基本思想是用多个处理器来共同求解同一问题，即将被求解的问题分解成若干个部分，各部分均由一个独立的处理机来并行计算。

并行计算可分为**时间上的并行**和**空间上的并行**。

**GPU全称为Graphics Processing Unit**，中文为**图形处理器**，就如它的名字一样，GPU最初是用在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上运行绘图运算工作的微处理器。

为什么GPU特别擅长处理图像数据呢？这是因为图像上的每一个像素点都有被处理的需要，而且每个像素点处理的过程和方式都十分相似，也就成了GPU的天然温床。

GPU简单架构如下图所示：

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/e7ac26fc862a469983022f5078cf4bbb.png)

图：GPU微架构示意图

**从架构图我们就能很明显的看出，GPU的构成相对简单，有数量众多的计算单元和超长的流水线，特别适合处理大量的类型统一的数据。**

**但GPU无法单独工作，必须由CPU进行控制调用才能工作**。CPU可单独作用，处理复杂的逻辑运算和不同的数据类型，但当需要大量的处理类型统一的数据时，则可调用GPU进行并行计算。

注：GPU中有很多的运算器ALU和很少的缓存cache，缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为线程thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram。

再把CPU和GPU两者放在一张图上看下对比，就非常一目了然了。

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/e0eed05169b64356a54dac42bcfaebcc.jpeg)

GPU的工作大部分都计算量大，但没什么技术含量，而且要重复很多很多次。

借用知乎上某大神的说法，就像你有个工作需要计算几亿次一百以内加减乘除一样，最好的办法就是雇上几十个小学生一起算，一人算一部分，反正这些计算也没什么技术含量，纯粹体力活而已；而CPU就像老教授，积分微分都会算，就是工资高，一个老教授资顶二十个小学生，你要是富士康你雇哪个？

GPU就是用很多简单的计算单元去完成大量的计算任务，纯粹的人海战术。这种策略基于一个前提，就是小学生A和小学生B的工作没有什么依赖性，是互相独立的。

**但有一点需要强调，虽然GPU是为了图像处理而生的，但是我们通过前面的介绍可以发现，它在结构上并没有专门为图像服务的部件**，只是对CPU的结构进行了优化与调整，所以现在GPU不仅可以在图像处理领域大显身手，它还被用来科学计算、密码破解、数值分析，海量数据处理（排序，Map-Reduce等），金融分析等需要大规模并行计算的领域。

所以GPU也可以认为是一种较通用的芯片。



#### 张量先锋：TPU

CPU和GPU都是较为通用的芯片，但是有句老话说得好：**万能工具的效率永远比不上专用工具。**

随着人们的计算需求越来越专业化，人们希望有芯片可以更加符合自己的专业需求，这时，便产生了ASIC（专用集成电路）的概念。

ASIC是指依产品需求不同而定制化的特殊规格集成电路，由特定使用者要求和特定电子系统的需要而设计、制造。当然这概念不用记，简单来说就是**定制化芯片。**

**因为ASIC很“专一”，只做一件事，所以它就会比CPU、GPU等能做很多件事的芯片在某件事上做的更好，实现更高的处理速度和更低的能耗。但相应的，ASIC的生产成本也非常高。**

而**TPU（Tensor Processing Unit, 张量处理器）**就是**谷歌**专门为加速深层神经网络运算能力而研发的一款芯片，**其实也是一款ASIC。**

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/1b9a0bc769184d02b5e29822b56bc8ea.jpeg)

图：谷歌第二代TPU

一般公司是很难承担为深度学习开发专门ASIC芯片的成本和风险的，但谷歌不会。更重要的原因是谷歌提供的很多服务，包括谷歌图像搜索、谷歌照片、谷歌云视觉API、谷歌翻译等产品和服务都需要用到深度神经网络。基于谷歌自身庞大的体量，开发一种专门的芯片开始具备规模化应用（大量分摊研发成本）的可能。

如此看来，TPU登上历史舞台也顺理成章了。

原来很多的机器学习以及图像处理算法大部分都跑在GPU与FPGA（半定制化芯片）上面，但这两种芯片都还是一种通用性芯片，所以在效能与功耗上还是不能更紧密的适配机器学习算法，而且Google一直坚信伟大的软件将在伟大的硬件的帮助下更加大放异彩，所以Google便想，我们可不可以做出一款专用机机器学习算法的专用芯片，TPU便诞生了。

据称，**TPU与同期的CPU和GPU相比，可以提供15-30倍的性能提升，以及30-80倍的效率（性能/瓦特）提升。**初代的TPU只能做推理，要依靠Google云来实时收集数据并产生结果，而训练过程还需要额外的资源；而第二代TPU既可以用于训练神经网络，又可以用于推理。

看到这里你可能会问了，为什么TPU会在性能上这么牛逼呢？

嗯，谷歌写了好几篇论文和博文来说明这一原因，所以仅在这里抛砖引玉一下。

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/947a5391ed5e417587395e1d08104a10.jpeg)

图：TPU 各模块的框图

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/cc49dbed99cf40238ba6f5b66dcdfeb4.jpeg)

图：TPU芯片布局图

如上图所示，TPU在芯片上使用了高达24MB的局部内存，6MB的累加器内存以及用于与主控处理器进行对接的内存，总共占芯片面积的37%（图中蓝色部分）。

**这表示谷歌充分意识到了片外内存访问是GPU能效比低的罪魁祸首，因此不惜成本的在芯片上放了巨大的内存。**相比之下，英伟达同时期的K80只有8MB的片上内存，因此需要不断地去访问片外DRAM。

另外，**TPU的高性能还来源于对于低运算精度的容忍。**研究结果表明，低精度运算带来的算法准确率损失很小，但是在硬件实现上却可以带来巨大的便利，包括功耗更低、速度更快、占芯片面积更小的运算单元、更小的内存带宽需求等...TPU采用了8比特的低精度运算。

到目前为止，TPU其实已经干了很多事情了，例如机器学习人工智能系统RankBrain，它是用来帮助Google处理搜索结果并为用户提供更加相关搜索结果的；还有街景Street View，用来提高地图与导航的准确性的；当然还有下围棋的计算机程序AlphaGo！



#### 仿神经元：NPU

**NPU（Neural network Processing Unit）**， 即**神经网络处理器**。顾名思义，这家伙是想用电路模拟人类的神经元和突触结构啊！

怎么模仿？那就得先来看看人类的神经结构——生物的神经网络由若干人工神经元结点互联而成，神经元之间通过突触两两连接，突触记录了神经元之间的联系。

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/4d8d2357f64e4408944db0da281dd46b.jpeg)

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/8033b5c1a83e437981067c3882478d90.jpeg)

如果想用电路模仿人类的神经元，就得把每个神经元抽象为一个激励函数，该函数的输入由与其相连的神经元的输出以及连接神经元的突触共同决定。

为了表达特定的知识，使用者通常需要（通过某些特定的算法）调整人工神经网络中突触的取值、网络的拓扑结构等。该过程称为“学习”。

在学习之后，人工神经网络可通过习得的知识来解决特定的问题。

这时不知道大家有没有发现问题——原来，由于深度学习的基本操作是神经元和突触的处理，而传统的处理器指令集（包括x86和ARM等）是为了进行通用计算发展起来的，其基本操作为算术操作（加减乘除）和逻辑操作（与或非），往往需要数百甚至上千条指令才能完成一个神经元的处理，深度学习的处理效率不高。

**这时就必须另辟蹊径——突破经典的冯·诺伊曼结构！**

神经网络中存储和处理是一体化的，都是通过突触权重来体现。 而冯·诺伊曼结构中，存储和处理是分离的，分别由存储器和运算器来实现，二者之间存在巨大的差异。当用现有的基于冯·诺伊曼结构的经典计算机（如X86处理器和英伟达GPU）来跑神经网络应用时，就不可避免地受到存储和处理分离式结构的制约，因而影响效率。这也就是专门针对人工智能的专业芯片能够对传统芯片有一定先天优势的原因之一。

**NPU的典型代表有国内的寒武纪芯片和IBM的TrueNorth。**以中国的寒武纪为例，DianNaoYu指令直接面对大规模神经元和突触的处理，一条指令即可完成一组神经元的处理，并对神经元和突触数据在芯片上的传输提供了一系列专门的支持。

用数字来说话，CPU、GPU与NPU相比，会有百倍以上的性能或能耗比差距——以寒武纪团队过去和Inria联合发表的DianNao论文为例——DianNao为单核处理器，主频为0.98GHz，峰值性能达每秒4520亿次神经网络基本运算，65nm工艺下功耗为0.485W，面积3.02平方毫米mm。

![img](http://5b0988e595225.cdn.sohucs.com/images/20171028/73d77b1268a04b45968d8a5cfba4b6f0.jpeg)

mate10中所用的麒麟970芯片，就集成了寒武纪的NPU，所以才可以实现所谓的照片优化功能，以及保证你的手机用了很长时间后还能不卡。

PS，中星微电子的“星光智能一号”虽说对外号称是NPU，但其实只是DSP，仅支持网络正向运算，无法支持神经网络训练。



### 百家争鸣：

**APU -- Accelerated Processing Unit, 加速处理器**，AMD公司推出加速图像处理芯片产品。

**BPU -- Brain Processing Unit**, 地平线公司主导的嵌入式处理器架构。

**CPU -- Central Processing Unit 中央处理器**， 目前PC core的主流产品。

**DPU -- Deep learning Processing Unit, 深度学习处理器**，最早由国内深鉴科技提出；另说有Dataflow Processing Unit 数据流处理器， Wave Computing 公司提出的AI架构；Data storage Processing Unit，深圳大普微的智能固态硬盘处理器。

**FPU -- Floating Processing Unit 浮点计算单元**，通用处理器中的浮点运算模块。

**GPU -- Graphics Processing Unit, 图形处理器**，采用多线程SIMD架构，为图形处理而生。

**HPU -- Holographics Processing Unit 全息图像处理器**， 微软出品的全息计算芯片与设备。

**IPU -- Intelligence Processing Unit**， Deep Mind投资的Graphcore公司出品的AI处理器产品。

**MPU/MCU -- Microprocessor/Micro controller Unit**， 微处理器/微控制器，一般用于低计算应用的RISC计算机体系架构产品，如ARM-M系列处理器。

**NPU -- Neural Network Processing Unit**，神经网络处理器，是基于神经网络算法与加速的新型处理器总称，如中科院计算所/寒武纪公司出品的diannao系列。

**RPU -- Radio Processing Unit, 无线电处理器**， Imagination Technologies 公司推出的集合集Wifi/蓝牙/FM/处理器为单片的处理器。

**TPU -- Tensor Processing Unit 张量处理器**， Google 公司推出的加速人工智能算法的专用处理器。目前一代TPU面向Inference，二代面向训练。

**VPU -- Vector Processing Unit 矢量处理器**，Intel收购的Movidius公司推出的图像处理与人工智能的专用芯片的加速计算核心。

**WPU -- Wearable Processing Unit， 可穿戴处理器**，Ineda Systems公司推出的可穿戴片上系统产品，包含GPU/MIPS CPU等IP。

**XPU -- 百度与Xilinx公司在2017年Hotchips大会上发布的FPGA智能云加速**，含256核。

**ZPU -- Zylin Processing Unit, **由挪威Zylin 公司推出的一款32位开源处理器。



### xPU:发展与未来

未来不同的AI芯片处理将继续井喷式的增长，据专家估计，只需要18天就有一个新的xPU诞生。根据发展的态势，未来的xPU将逐渐趋向专一化，每一个芯片用于处理特定的场合。未来处理器中将会结合多种AI芯片，由一个调度模块对各个模块分配任务。每个AI芯片各司其职，全面提升pc和服务器的性能。随着工艺的越发精进，越来越多的模块被整合到芯片之中。芯片的功能将会变得更加强大，

当然，由于不同的xPU专精某一个方面，所以单独的一个芯片将无法工作，比如GPU就要依赖CPU才能正常工作，所以如何才能完美的整合不同的AI芯片是未来的芯片发展的难题。我觉得如果定下统一的接口标准，不同的芯片按照特定的标准进行开发，将大大减轻芯片交互的问题，未来一端多芯将触手可及。

未来AI芯片必将百花齐放！